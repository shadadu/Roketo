{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af1d1e6-3856-4c06-b4ff-4233f6b6c93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "import html\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import xml.etree.ElementTree as ET\n",
    "from transformers import pipeline\n",
    "import sentencepiece\n",
    "import pandas as pd\n",
    "import fitz  # PyMuPDF\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import io\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fbfc0f-9e6d-45cd-bd54-bdb2775a7231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the question generation pipeline\n",
    "qg_pipeline = pipeline(\"text2text-generation\", model=\"valhalla/t5-base-qg-hl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff6a51c-82e3-45a3-87d3-a55520096e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(content[0:50000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be774ca-fdc2-41f8-b3f3-4e204fd7a8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_regex_paragraphs(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    text = re.sub(r\"\\bFigure\\s+\\d+-\\d+\\b\", \"\", text, flags=re.IGNORECASE)\n",
    "    # Remove any number surrounded by spaces (e.g., \" 42 \")\n",
    "    text = re.sub(r\"\\d+\", \" \", text)\n",
    "    # Optional: strip redundant whitespace\n",
    "    cleaned_text = re.sub(r\"\\s{2,}\", \" \", text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6ed7a1-82f1-471c-a94b-85f5c14157ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_split_paragraphs(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Split on one or more blank lines (handles multiple \\n between paragraphs)\n",
    "    import re\n",
    "    pattern = r\"(*Figure -*\\s*\\s*-\\s*(\\d+)\\s*\"\n",
    "    pat = r\"[ ]*(\\d+[ ]*)|\\(*Figure -*\\s*\\d+\\)?\"\n",
    "    # paragraphs = [para.strip() for para in re.split(r'\\n\\s*\\n', re.sub(pattern, \" \", text)) if para.strip()]\n",
    "    paragraphs = [para.strip().replace(\"- . \",\"\").replace(\" . \",\"\") for para in re.split(r'\\n\\s*\\n', re.sub(r\"\\s*\\d+\\s*\", \"\", re.sub(pat, \" \", text))) if para.strip()]\n",
    "    \n",
    "    return paragraphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e44546-8bbc-46e9-baad-4dfab4be91e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "paragraphs = load_and_split_paragraphs(\"/Users/rckyi/Documents/Data/Nasa-Lessons-learned-in-engineering.txt\")\n",
    "for i, para in enumerate(paragraphs[:500]):  # Print first 5 paragraph\n",
    "    input_text = f\"generate question: {para}\"\n",
    "    generated_question = qg_pipeline(input_text, max_length=64, do_sample=False)[0]['generated_text']\n",
    "    print(f\"\\n--- Paragraph {i + 1} ---\\n{generated_question}\\n{para}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44c1b62-0e7f-49ab-af2c-2f133e282b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def func(paragraph):\n",
    "    # Example placeholder function â€” replace this with your desired logic\n",
    "    return len(paragraph.split())  # returns word count\n",
    "\n",
    "def process_paragraphs(json_file_path):\n",
    "    with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "        paragraphs = json.load(f)\n",
    "    \n",
    "    for para_id, para_text in paragraphs.items():\n",
    "        input_text = f\"generate question: {para_text}\"\n",
    "        output = qg_pipeline(input_text, max_length=64, do_sample=False)[0]['generated_text']\n",
    "        yield [qg_pipeline(input_text, max_length=512, do_sample=False)[0]['generated_text'], para_text]\n",
    "\n",
    "# Example usage:\n",
    "json_file_path = \"/Users/rckyi/Documents/Data/paragraphs_with_ids.json\"  # or your full path\n",
    "results = list(process_paragraphs(json_file_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfd3176-b30a-44e3-bb28-fea4be34640c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from happytransformer import HappyTextToText, TTSettings\n",
    "\n",
    "happy_tt = HappyTextToText(\"T5\", \"vennify/t5-base-grammar-correction\")\n",
    "\n",
    "args = TTSettings(num_beams=5, min_length=1)\n",
    "\n",
    "# Add the prefix \"grammar: \" before each input \n",
    "result = happy_tt.generate_text(\"grammar: This sentences has has bads grammar.\", args=args)\n",
    "\n",
    "print(result.text) # This sentence has bad grammar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc64a7e8-10a5-4ba2-b693-46dee32e1e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see the first few results:\n",
    "for pair in results[500:505]:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8ec14a-8ad7-4575-869b-fc28758273b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrector(\"fix: posttestinspectionandengineteardownrevealedmechanicalfailure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7f9408-8d03-467c-bfb8-bf309ec2b4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "# Load the JSON paragraphs file\n",
    "with open(\"/Users/rckyi/Documents/Data/paragraphs_with_ids.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    paragraphs = json.load(f)\n",
    "\n",
    "# Initialize the spell checker\n",
    "spell = SpellChecker()\n",
    "\n",
    "# Function to attempt splitting a word into valid parts\n",
    "def split_word(word):\n",
    "    word = word.lower()\n",
    "    if word in spell:\n",
    "        return word\n",
    "    for i in range(3, len(word) - 3):\n",
    "        prefix, suffix = word[:i], word[i:]\n",
    "        if prefix in spell and suffix in spell:\n",
    "            return prefix + ' ' + suffix\n",
    "    return word\n",
    "\n",
    "# Function to correct concatenated words in a paragraph\n",
    "def correct_paragraph(paragraph):\n",
    "    corrected_tokens = []\n",
    "    for token in re.findall(r'\\w+|\\W+', paragraph):\n",
    "        if token.strip().isalpha():\n",
    "            corrected_tokens.append(split_word(token))\n",
    "        else:\n",
    "            corrected_tokens.append(token)\n",
    "    return ''.join(corrected_tokens)\n",
    "\n",
    "# Apply correction to all paragraphs\n",
    "corrected_lines = []\n",
    "for pid, para in paragraphs.items():\n",
    "    # cleaned = correct_paragraph(para)\n",
    "    cleaned = corrector(f\"fix: +{para}\")\n",
    "    corrected_lines.append(f\"{pid}:\\n{cleaned}\\n\")\n",
    "\n",
    "# print(f'corrected lines {corrected_lines}')\n",
    "\n",
    "# Write to output text file\n",
    "with open(\"/Users/rckyi/Documents/Data/corrected_paragraphs_output.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.writelines(corrected_lines)\n",
    "\n",
    "print(\"Corrected text saved to 'corrected_paragraphs_output.txt'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fd6995-9d71-4bbc-a7a7-012c01d09a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "# Load the JSON paragraphs file\n",
    "with open(\"/Users/rckyi/Documents/Data/paragraphs_with_ids.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    paragraphs = json.load(f)\n",
    "\n",
    "# Initialize the spell checker\n",
    "spell = SpellChecker()\n",
    "\n",
    "# Function to attempt splitting a word into valid parts\n",
    "def split_word(word):\n",
    "    word = word.lower()\n",
    "    if word in spell:\n",
    "        return word\n",
    "    for i in range(3, len(word) - 3):\n",
    "        prefix, suffix = word[:i], word[i:]\n",
    "        if prefix in spell and suffix in spell:\n",
    "            return prefix + ' ' + suffix\n",
    "    return word\n",
    "\n",
    "# Function to correct concatenated words in a paragraph\n",
    "def correct_paragraph(paragraph):\n",
    "    corrected_tokens = []\n",
    "    for token in re.findall(r'\\w+|\\W+', paragraph):\n",
    "        if token.strip().isalpha():\n",
    "            corrected_tokens.append(split_word(token))\n",
    "        else:\n",
    "            corrected_tokens.append(token)\n",
    "    return ''.join(corrected_tokens)\n",
    "\n",
    "# Apply correction to all paragraphs\n",
    "corrected_dict = {}\n",
    "for pid, para in paragraphs.items():\n",
    "    corrected_dict[pid] = correct_paragraph(para)\n",
    "\n",
    "# Save the corrected paragraphs to JSON\n",
    "with open(\"/Users/rckyi/Documents/Data/corrected_paragraphs_output.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(corrected_dict, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"Corrected paragraphs saved to 'corrected_paragraphs_output.json'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8ab7b4-dcdc-44fa-a66b-e7415c9d6c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_word(\"posttestinspectionandengineteardownrevealedmechanicalfailure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7b4610-b707-4be9-8e2f-340d41aedab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_impulse_classes():\n",
    "   \n",
    "    # URL of the Wikipedia page\n",
    "    url = \"https://en.wikipedia.org/wiki/Model_rocket_motor_classification\"\n",
    "    \n",
    "    # Send a request to the webpage\n",
    "    response = requests.get(url)\n",
    "    # print(f'response json {response.json()}')\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    # print(f'soup {soup}')\n",
    "    \n",
    "    # Find all tables\n",
    "    tables = soup.find_all('table', {'class': 'wikitable'})\n",
    "    # imp = soup.find_all('ta')\n",
    "    # print(f'table {tables}')\n",
    "    \n",
    "    # Look for the table that contains the header \"Total impulse (NÂ·s)\"\n",
    "    target_table = None\n",
    "    \n",
    "    for table in tables:\n",
    "        # print(f'table text {table.}')\n",
    "        if 'impulse' in table.text:\n",
    "            print('found')\n",
    "            target_table = table\n",
    "            break\n",
    "    # print(target_table)\n",
    "    # Parse the table into a DataFrame\n",
    "    df = pd.read_html(str(tables))[0]\n",
    "    \n",
    "    # Display the resulting DataFrame\n",
    "    print(df['Class  (Base 26)'])\n",
    "    ls = df['Class  (Base 26)'].tolist()\n",
    "    \n",
    "    return [x for x in ls if len(x) < 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e31ad57-2f6b-4614-b8d1-36be91d8350d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack Exchange API\n",
    "# def fetch_stackexchange_qa(site='space', tagged='rocket', pages=5):\n",
    "#     questions = []\n",
    "#     for page in range(1, pages+1):\n",
    "#         url = f\"https://api.stackexchange.com/2.3/questions?page={page}&pagesize=20&order=desc&sort=activity&tagged={tagged}&site={site}&filter=withbody\"\n",
    "#         resp = requests.get(url).json()\n",
    "#         print(f'resp {resp}')\n",
    "#         for item in resp.get('items', []):\n",
    "#             q = {\n",
    "#                 'question': item['title'],\n",
    "#                 'body': item.get('body', ''),\n",
    "#                 'answers': []\n",
    "#             }\n",
    "#             if item.get('is_answered'):\n",
    "#                 a_url = f\"https://api.stackexchange.com/2.3/questions/{item['question_id']}/answers?order=desc&sort=votes&site={site}&filter=withbody\"\n",
    "#                 a_resp = requests.get(a_url).json()\n",
    "#                 for answer in a_resp.get('items', []):\n",
    "#                     q['answers'].append(answer['body'])\n",
    "#                     questions.append(q)\n",
    "#             # questions.append(q)\n",
    "#         time.sleep(1)  # Respect rate limits\n",
    "#     return questions\n",
    "\n",
    " \n",
    "# arXiv API Q&A extraction from abstract (pseudo-QA from title and abstract)\n",
    "def fetch_arxiv_abstracts(query='rocket propulsion', max_results=10):\n",
    "    url = f\"http://export.arxiv.org/api/query?search_query=all:{query.replace(' ', '+')}&start=0&max_results={max_results}\"\n",
    "    resp = requests.get(url)\n",
    "    root = ET.fromstring(resp.content)\n",
    "\n",
    "    qas = []\n",
    "    ns = {'atom': 'http://www.w3.org/2005/Atom'}  # arXiv uses Atom XML namespace\n",
    "\n",
    "    for entry in root.findall('atom:entry', ns):\n",
    "        title = entry.find('atom:title', ns).text.strip()\n",
    "        \n",
    "        summary = entry.find('atom:summary', ns).text.strip()\n",
    "        # Generate question from text\n",
    "        input_text = f\"generate question: {summary}\"\n",
    "        generated_question = qg_pipeline(input_text, max_length=64, do_sample=False)[0]['generated_text']\n",
    "        \n",
    "        qas.append({\n",
    "            'title': title,\n",
    "            'question': generated_question,\n",
    "            'answer': summary\n",
    "        })\n",
    "    return qas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46cadbc-7b22-4282-885d-7b7867293821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wikipedia scraping\n",
    "\n",
    "# Initialize the question generation pipeline\n",
    "# qg_pipeline = pipeline(\"text2text-generation\", model=\"valhalla/t5-base-qg-hl\")\n",
    "\n",
    "def clean_text(text):\n",
    "    return ' '.join(text.strip().split())\n",
    "\n",
    "def get_all_rocket_propulsion_links(base_url='https://en.wikibooks.org/wiki/Rocket_Propulsion'):\n",
    "    \"\"\"Grab all unique subpage links under Rocket Propulsion.\"\"\"\n",
    "    resp = requests.get(base_url)\n",
    "    soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "\n",
    "    content_div = soup.select_one('#mw-content-text')\n",
    "    links = content_div.find_all('a', href=True)\n",
    "    \n",
    "    urls = set()\n",
    "    for link in links:\n",
    "        href = link['href']\n",
    "        if 'https' in href:\n",
    "            full_url = href\n",
    "        else:\n",
    "            full_url = 'https://en.wikibooks.org' + href\n",
    "            # print(f'full_url {full_url}')\n",
    "        urls.add(full_url)\n",
    "        # if href.startswith('/wiki/Rocket_Propulsion/') and ':' not in href:\n",
    "        #     print(f'url: https://en.wikibooks.org + { href}')\n",
    "        #     urls.add('https://en.wikibooks.org' + href)\n",
    "\n",
    "    return list(urls)\n",
    "\n",
    "def scrape_pages_with_qg(urls, visited, M, batch_size=10):\n",
    "    \"\"\"Scrape up to `batch_size` new pages not in `visited`, return new QAs.\"\"\"\n",
    "    qa_pairs = []\n",
    "    count = 0\n",
    "    print(f'visited {visited}')\n",
    "\n",
    "    for url in urls:\n",
    "        if url in visited:\n",
    "            continue\n",
    "        visited.add(url)\n",
    "        try:\n",
    "            resp = requests.get(url)\n",
    "            soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "            content_div = soup.select_one('#mw-content-text')\n",
    "            paragraphs = content_div.find_all('p')\n",
    "            text = clean_text(' '.join(p.get_text() for p in paragraphs[:3]))\n",
    "            if not text or len(text) < M:\n",
    "                continue\n",
    "\n",
    "            # Generate question from text\n",
    "            input_text = f\"generate question: {text}\"\n",
    "            output = qg_pipeline(input_text, max_length=64, do_sample=False)[0]['generated_text']\n",
    "\n",
    "            qa_pairs.append({\n",
    "                'question': output,\n",
    "                'answer': text,\n",
    "                'source_url': url\n",
    "            })\n",
    "            print(f\"âœ… Generated Q&A from: {url}\")\n",
    "            count += 1\n",
    "            time.sleep(1)  # polite scraping\n",
    "\n",
    "            if count >= batch_size:\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Failed to process {url}: {e}\")\n",
    "    return qa_pairs\n",
    "\n",
    "def scrape_wikibook_qas(M):\n",
    "    all_links = get_all_rocket_propulsion_links()\n",
    "    visited = set()\n",
    "    all_qas = []\n",
    "\n",
    "    for i in range(10):\n",
    "        print(f\"\\nðŸ” Batch {i+1}/10\")\n",
    "        batch_qas = scrape_pages_with_qg(all_links, visited, M, batch_size=10)\n",
    "        all_qas.extend(batch_qas)\n",
    "        if len(all_qas) >= M:\n",
    "            break\n",
    "\n",
    "    # with open('wikibook_qas_100.json', 'w', encoding='utf-8') as f:\n",
    "    #     json.dump(all_qas, f, indent=2)\n",
    "\n",
    "    print(f\"\\nâœ… Done. Saved {len(all_qas)} Q&A pairs to wikibook_qas_100.json\")\n",
    "    return all_qas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719f642f-6bb1-4cb2-8c60-da96865f386e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack Exchange API\n",
    "\n",
    "def clean_html_text(html_content):\n",
    "    \"\"\"Remove hyperlinks and strip HTML tags from content.\"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Replace <a> tags with their inner text\n",
    "    for a in soup.find_all('a'):\n",
    "        a.replace_with(a.get_text())\n",
    "\n",
    "    # Get cleaned text\n",
    "    text = soup.get_text(separator=' ')\n",
    "    return html.unescape(text.strip())\n",
    "\n",
    "def fetch_stackexchange_qa_data(site='space.stackexchange', tag='rockets', pagesize=20, max_pages=2):\n",
    "    base_url = 'https://api.stackexchange.com/2.3/questions'\n",
    "    answers_url = 'https://api.stackexchange.com/2.3/questions/{ids}/answers'\n",
    "    all_qas = []\n",
    "\n",
    "    for page in range(1, max_pages + 1):\n",
    "        params = {\n",
    "            'site': site,\n",
    "            'tagged': tag,\n",
    "            'pagesize': pagesize,\n",
    "            'page': page,\n",
    "            'filter': 'withbody'\n",
    "        }\n",
    "        resp = requests.get(base_url, params=params)\n",
    "        data = resp.json()\n",
    "\n",
    "        for question in data.get('items', []):\n",
    "            q_id = question['question_id']\n",
    "            q_body = clean_html_text(question.get('body', ''))\n",
    "            title = html.unescape(question.get('title', ''))\n",
    "\n",
    "            # Get answers\n",
    "            a_params = {\n",
    "                'site': site,\n",
    "                'filter': 'withbody'\n",
    "            }\n",
    "            a_resp = requests.get(answers_url.format(ids=q_id), params=a_params)\n",
    "            answers = a_resp.json().get('items', [])\n",
    "\n",
    "            for ans in answers:\n",
    "                a_body = clean_html_text(ans.get('body', ''))\n",
    "                all_qas.append({\n",
    "                    'question': f\"{title}\\n{q_body}\",\n",
    "                    'answer': a_body,\n",
    "                    'source_url': f\"https://{site}.com/questions/{q_id}\"\n",
    "                })\n",
    "\n",
    "            time.sleep(1)  # polite API usage\n",
    "\n",
    "    return all_qas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c3507a-3c15-442f-8d5f-7b0a0ce55bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    se_data = fetch_stackexchange_qa_data()\n",
    "    # arxiv_data = fetch_arxiv_abstracts()\n",
    "    # reddit_data = fetch_reddit_qa()\n",
    "    # wikibook_data = scrape_wikibook_qas(M=100)\n",
    "\n",
    "    # all_qas = {\n",
    "    #     \"stackexchange\": se_data,\n",
    "    #     \"arxiv\": arxiv_data,\n",
    "    #     \"reddit\": reddit_data,\n",
    "    #     \"wikibook\": wikibook_data\n",
    "    # }\n",
    "\n",
    "    # save_qas_to_file(all_qas)\n",
    "    # print(\"Data collection complete. Saved to qa_data.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bae52e4-b8d2-4b66-a834-8ba3d5c432cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# se_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99c21fa-8f6e-4c3d-8b32-f63e5f50ee22",
   "metadata": {},
   "outputs": [],
   "source": [
    "se_data[12]['question']#.split('\\n')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f894c2-cae5-49a4-9fcf-0eb77841f13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "se_data[12]['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca48da5-bcd2-4151-9aab-3ebd3fae4875",
   "metadata": {},
   "outputs": [],
   "source": [
    "se_data[1][\"body\"].replace(\"<p>\",\"\").replace('\"','').replace(\"</blockquote>\",\" \").replace(\"<a>\",\"\").replace(\"<blockquote>\",\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b4eace-6457-4b98-8b69-7488d1699ee9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_torch",
   "language": "python",
   "name": "gpu_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
