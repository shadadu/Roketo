{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af1d1e6-3856-4c06-b4ff-4233f6b6c93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "import html\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import xml.etree.ElementTree as ET\n",
    "from transformers import pipeline\n",
    "import sentencepiece\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import io\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fbfc0f-9e6d-45cd-bd54-bdb2775a7231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the question generation pipeline\n",
    "qg_pipeline = pipeline(\"text2text-generation\", model=\"valhalla/t5-base-qg-hl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a143cd1-bbf5-4b2a-9475-fb69c6df7490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_split_sentences(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        sentences = f.readlines()\n",
    "    qas = []\n",
    "    for c, item in enumerate(sentences[0:3500]):\n",
    "        input_text = f\"generate question: {item}\"\n",
    "        generated_question = qg_pipeline(input_text, max_length=512, do_sample=False)[0]['generated_text']\n",
    "        yield {\n",
    "            'question': generated_question,\n",
    "            'answer': item,\n",
    "            'source': 'SpaceSystemsDataset'\n",
    "        }\n",
    "        # print(f'{c} -> /n {generated_question}/n {item}')\n",
    "    # return qas\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2b9994-4945-44b7-99e1-e8cbe260476b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# space_systems_qas = list(load_and_split_sentences('/Users/rckyi/Documents/Data/SpaceSystemsDataset/2 SpaceTransformersCorpus/Sentences_WikiBooksAbstracts.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea372cb-9717-4bd7-b5a8-ac3c123d3d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(space_systems_qas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6ed7a1-82f1-471c-a94b-85f5c14157ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_split_paragraphs(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Split on one or more blank lines (handles multiple \\n between paragraphs)\n",
    "    import re\n",
    "    pattern = r\"(*Figure -*\\s*\\s*-\\s*(\\d+)\\s*\"\n",
    "    pat = r\"[ ]*(\\d+[ ]*)|\\(*Figure -*\\s*\\d+\\)?\"\n",
    "    # paragraphs = [para.strip() for para in re.split(r'\\n\\s*\\n', re.sub(pattern, \" \", text)) if para.strip()]\n",
    "    paragraphs = [para.strip().replace(\"- . \",\"\").replace(\" . \",\"\") for para in re.split(r'\\n\\s*\\n', re.sub(r\"\\s*\\d+\\s*\", \"\", re.sub(pat, \" \", text))) if para.strip()]\n",
    "    \n",
    "    return paragraphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e44546-8bbc-46e9-baad-4dfab4be91e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "paragraphs = load_and_split_paragraphs(\"/Users/rckyi/Documents/Data/Nasa-Lessons-learned-in-engineering.txt\")\n",
    "\n",
    "lessons_learned_qa = []\n",
    "for i, para in enumerate(paragraphs):  # Print first 5 paragraph\n",
    "    input_text = f\"generate question: {para}\"\n",
    "    generated_question = qg_pipeline(input_text, max_length=512, do_sample=False)[0]['generated_text']\n",
    "    lessons_learned_qa.append({\n",
    "            'question': generated_question,\n",
    "            'answer': para,\n",
    "            'source': 'nasa: lessons learned'\n",
    "    })\n",
    "    \n",
    "    # print(f\"\\n--- Paragraph {i + 1} ---\\n{generated_question}\\n{para}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19289fc8-e301-4cbe-b0b9-65d5a8e8ac0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paragraphs_ps = split_into_paragraphs(\"/Users/rckyi/Documents/Data/A HISTORY OF AEROSPACE PROBLEMS, THEIR SOLUTIONS, THEIR LESSONS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46c00f8-58a3-4898-bbaa-7204b3db035b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, para in enumerate(paragraphs_ps[200:300]):  # Print first 5 paragraph\n",
    "#     input_text = f\"generate question: {para}\"\n",
    "#     generated_question = qg_pipeline(input_text, max_length=512, do_sample=False)[0]['generated_text']\n",
    "#     print(f\"\\n--- Paragraph {i + 1} ---\\n{generated_question}\\n{para}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44c1b62-0e7f-49ab-af2c-2f133e282b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_paragraphs(json_file_path):\n",
    "    with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "        paragraphs = json.load(f)\n",
    "\n",
    "    results = []\n",
    "    for para_id, para_text in paragraphs.items():\n",
    "        input_text = f\"generate question: {para_text}\"\n",
    "        results.append({'question': qg_pipeline(input_text, max_length=712, do_sample=False)[0]['generated_text'], \n",
    "                        'answer': para_text,\n",
    "                       'source':  f'nasa: a history of aerospace problems and solns'\n",
    "                       })\n",
    "        # yield [qg_pipeline(input_text, max_length=512, do_sample=False)[0]['generated_text'], para_text]\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8ab7b4-dcdc-44fa-a66b-e7415c9d6c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "json_file_path = \"/Users/rckyi/Documents/Data/paragraphs_with_ids.json\"  # or your full path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e31ad57-2f6b-4614-b8d1-36be91d8350d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arXiv API Q&A extraction from abstract (pseudo-QA from title and abstract)\n",
    "def fetch_arxiv_abstracts(query='rocket propulsion', max_results=100):\n",
    "    url = f\"http://export.arxiv.org/api/query?search_query=all:{query.replace(' ', '+')}&start=0&max_results={max_results}\"\n",
    "    resp = requests.get(url)\n",
    "    root = ET.fromstring(resp.content)\n",
    "\n",
    "    qas = []\n",
    "    ns = {'atom': 'http://www.w3.org/2005/Atom'}  # arXiv uses Atom XML namespace\n",
    "\n",
    "    for entry in root.findall('atom:entry', ns):\n",
    "        title = entry.find('atom:title', ns).text.strip()\n",
    "        \n",
    "        summary = entry.find('atom:summary', ns).text.strip()\n",
    "        # Generate question from text\n",
    "        input_text = f\"generate question: {summary}\"\n",
    "        generated_question = qg_pipeline(input_text, max_length=64, do_sample=False)[0]['generated_text']\n",
    "        \n",
    "        qas.append({\n",
    "            # 'title': title,\n",
    "            'question': generated_question,\n",
    "            'answer': summary,\n",
    "            'source': f'arxiv'\n",
    "        })\n",
    "    return qas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46cadbc-7b22-4282-885d-7b7867293821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wikipedia scraping\n",
    "\n",
    "# Initialize the question generation pipeline\n",
    "# qg_pipeline = pipeline(\"text2text-generation\", model=\"valhalla/t5-base-qg-hl\")\n",
    "\n",
    "def clean_text(text):\n",
    "    return ' '.join(text.strip().split())\n",
    "\n",
    "def get_all_rocket_propulsion_links(base_url='https://en.wikibooks.org/wiki/Rocket_Propulsion'):\n",
    "    \"\"\"Grab all unique subpage links under Rocket Propulsion.\"\"\"\n",
    "    resp = requests.get(base_url)\n",
    "    soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "\n",
    "    content_div = soup.select_one('#mw-content-text')\n",
    "    links = content_div.find_all('a', href=True)\n",
    "    \n",
    "    urls = set()\n",
    "    for link in links:\n",
    "        href = link['href']\n",
    "        if 'https' in href:\n",
    "            full_url = href\n",
    "        else:\n",
    "            full_url = 'https://en.wikibooks.org' + href\n",
    "        urls.add(full_url)\n",
    "\n",
    "    return list(urls)\n",
    "\n",
    "def scrape_pages_with_qg(urls, visited, M, batch_size=10):\n",
    "    \"\"\"Scrape up to `batch_size` new pages not in `visited`, return new QAs.\"\"\"\n",
    "    qa_pairs = []\n",
    "    count = 0\n",
    "\n",
    "    for url in urls:\n",
    "        if url in visited:\n",
    "            continue\n",
    "        visited.add(url)\n",
    "        try:\n",
    "            resp = requests.get(url)\n",
    "            soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "            content_div = soup.select_one('#mw-content-text')\n",
    "            paragraphs = content_div.find_all('p')\n",
    "            text = clean_text(' '.join(p.get_text() for p in paragraphs[:3]))\n",
    "            if not text or len(text) < M:\n",
    "                continue\n",
    "\n",
    "            # Generate question from text\n",
    "            input_text = f\"generate question: {text}\"\n",
    "            output = qg_pipeline(input_text, max_length=64, do_sample=False)[0]['generated_text']\n",
    "\n",
    "            qa_pairs.append({\n",
    "                'question': output,\n",
    "                'answer': text,\n",
    "                'source': f\"rocketry wiki\"\n",
    "            })\n",
    "            print(f\"âœ… Generated Q&A from: {url}\")\n",
    "            count += 1\n",
    "            time.sleep(1)  # polite scraping\n",
    "\n",
    "            if count >= batch_size:\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Failed to process {url}: {e}\")\n",
    "    return qa_pairs\n",
    "\n",
    "def scrape_wikibook_qas(M):\n",
    "    all_links = get_all_rocket_propulsion_links()\n",
    "    visited = set()\n",
    "    all_qas = []\n",
    "\n",
    "    for i in range(10):\n",
    "        print(f\"\\nðŸ” Batch {i+1}/10\")\n",
    "        batch_qas = scrape_pages_with_qg(all_links, visited, M, batch_size=10)\n",
    "        all_qas.extend(batch_qas)\n",
    "        if len(all_qas) >= M:\n",
    "            break\n",
    "\n",
    "    return all_qas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719f642f-6bb1-4cb2-8c60-da96865f386e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack Exchange API\n",
    "\n",
    "def clean_html_text(html_content):\n",
    "    \"\"\"Remove hyperlinks and strip HTML tags from content.\"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Replace <a> tags with their inner text\n",
    "    for a in soup.find_all('a'):\n",
    "        a.replace_with(a.get_text())\n",
    "\n",
    "    # Get cleaned text\n",
    "    text = soup.get_text(separator=' ')\n",
    "    return html.unescape(text.strip())\n",
    "\n",
    "def fetch_stackexchange_qas(site='space.stackexchange', tag='rockets', pagesize=20, max_pages=10):\n",
    "    base_url = 'https://api.stackexchange.com/2.3/questions'\n",
    "    answers_url = 'https://api.stackexchange.com/2.3/questions/{ids}/answers'\n",
    "    all_qas = []\n",
    "\n",
    "    for page in range(1, max_pages + 1):\n",
    "        params = {\n",
    "            'site': site,\n",
    "            'tagged': tag,\n",
    "            'pagesize': pagesize,\n",
    "            'page': page,\n",
    "            'filter': 'withbody'\n",
    "        }\n",
    "        resp = requests.get(base_url, params=params)\n",
    "        print(resp)\n",
    "        data = resp.json()\n",
    "\n",
    "        for question in data.get('items', []):\n",
    "            q_id = question['question_id']\n",
    "            q_body = clean_html_text(question.get('body', ''))\n",
    "            title = html.unescape(question.get('title', ''))\n",
    "\n",
    "            # Get answers\n",
    "            a_params = {\n",
    "                'site': site,\n",
    "                'filter': 'withbody'\n",
    "            }\n",
    "            a_resp = requests.get(answers_url.format(ids=q_id), params=a_params)\n",
    "            answers = a_resp.json().get('items', [])\n",
    "\n",
    "            for ans in answers:\n",
    "                a_body = clean_html_text(ans.get('body', ''))\n",
    "                all_qas.append({\n",
    "                    'question': f\"{title}\\n{q_body}\",\n",
    "                    'answer': a_body,\n",
    "                    'source': f\"https://{site}.com/questions/{q_id}\"\n",
    "                })\n",
    "\n",
    "            time.sleep(20)  # polite API usage\n",
    "\n",
    "    return all_qas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfb7242-e0f0-4311-b725-cdd7bbd0f22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# se_data = fetch_stackexchange_qas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c3507a-3c15-442f-8d5f-7b0a0ce55bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    arxiv_data = fetch_arxiv_abstracts()\n",
    "    # reddit_data = fetch_reddit_qa()\n",
    "    wikibook_data = scrape_wikibook_qas(M=1000)\n",
    "    space_systems_qas = list(load_and_split_sentences('/Users/rckyi/Documents/Data/SpaceSystemsDataset/2 SpaceTransformersCorpus/Sentences_WikiBooksAbstracts.txt'))\n",
    "    # nasa_probs_solns = process_paragraphs(json_file_path)\n",
    "\n",
    "    all_qas = {\n",
    "        # \"stackexchange\": se_data,\n",
    "        \"arxiv\": arxiv_data,\n",
    "        # \"reddit\": reddit_data,\n",
    "        \"wikibook\": wikibook_data,\n",
    "        \"spacesystems\": space_systems_qas,\n",
    "        \"nasa lessons learned\": lessons_learned_qa\n",
    "        # \"nasa problems and solns\": nasa_probs_solns\n",
    "    }\n",
    "\n",
    "    with open('/Users/rckyi/Documents/Data/all_qas_dataset.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_qas, f, indent=2)\n",
    "\n",
    "    print(f\"\\nâœ… Done. Saved {len(all_qas)} Q&A pairs to qas_data.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afdf94d-3235-4eb2-8afb-6cbe5b1021ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_torch",
   "language": "python",
   "name": "gpu_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
