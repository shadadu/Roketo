{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5af1d1e6-3856-4c06-b4ff-4233f6b6c93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "import html\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import xml.etree.ElementTree as ET\n",
    "from transformers import pipeline\n",
    "import sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0fbfc0f-9e6d-45cd-bd54-bdb2775a7231",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the question generation pipeline\n",
    "qg_pipeline = pipeline(\"text2text-generation\", model=\"valhalla/t5-base-qg-hl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e31ad57-2f6b-4614-b8d1-36be91d8350d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack Exchange API\n",
    "# def fetch_stackexchange_qa(site='space', tagged='rocket', pages=5):\n",
    "#     questions = []\n",
    "#     for page in range(1, pages+1):\n",
    "#         url = f\"https://api.stackexchange.com/2.3/questions?page={page}&pagesize=20&order=desc&sort=activity&tagged={tagged}&site={site}&filter=withbody\"\n",
    "#         resp = requests.get(url).json()\n",
    "#         print(f'resp {resp}')\n",
    "#         for item in resp.get('items', []):\n",
    "#             q = {\n",
    "#                 'question': item['title'],\n",
    "#                 'body': item.get('body', ''),\n",
    "#                 'answers': []\n",
    "#             }\n",
    "#             if item.get('is_answered'):\n",
    "#                 a_url = f\"https://api.stackexchange.com/2.3/questions/{item['question_id']}/answers?order=desc&sort=votes&site={site}&filter=withbody\"\n",
    "#                 a_resp = requests.get(a_url).json()\n",
    "#                 for answer in a_resp.get('items', []):\n",
    "#                     q['answers'].append(answer['body'])\n",
    "#                     questions.append(q)\n",
    "#             # questions.append(q)\n",
    "#         time.sleep(1)  # Respect rate limits\n",
    "#     return questions\n",
    "\n",
    " \n",
    "# arXiv API Q&A extraction from abstract (pseudo-QA from title and abstract)\n",
    "def fetch_arxiv_abstracts(query='rocket propulsion', max_results=10):\n",
    "    url = f\"http://export.arxiv.org/api/query?search_query=all:{query.replace(' ', '+')}&start=0&max_results={max_results}\"\n",
    "    resp = requests.get(url)\n",
    "    root = ET.fromstring(resp.content)\n",
    "\n",
    "    qas = []\n",
    "    ns = {'atom': 'http://www.w3.org/2005/Atom'}  # arXiv uses Atom XML namespace\n",
    "\n",
    "    for entry in root.findall('atom:entry', ns):\n",
    "        title = entry.find('atom:title', ns).text.strip()\n",
    "        \n",
    "        summary = entry.find('atom:summary', ns).text.strip()\n",
    "        # Generate question from text\n",
    "        input_text = f\"generate question: {summary}\"\n",
    "        generated_question = qg_pipeline(input_text, max_length=64, do_sample=False)[0]['generated_text']\n",
    "        \n",
    "        qas.append({\n",
    "            'title': title,\n",
    "            'question': generated_question,\n",
    "            'answer': summary\n",
    "        })\n",
    "    return qas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a46cadbc-7b22-4282-885d-7b7867293821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wikipedia scraping\n",
    "\n",
    "# Initialize the question generation pipeline\n",
    "# qg_pipeline = pipeline(\"text2text-generation\", model=\"valhalla/t5-base-qg-hl\")\n",
    "\n",
    "def clean_text(text):\n",
    "    return ' '.join(text.strip().split())\n",
    "\n",
    "def get_all_rocket_propulsion_links(base_url='https://en.wikibooks.org/wiki/Rocket_Propulsion'):\n",
    "    \"\"\"Grab all unique subpage links under Rocket Propulsion.\"\"\"\n",
    "    resp = requests.get(base_url)\n",
    "    soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "\n",
    "    content_div = soup.select_one('#mw-content-text')\n",
    "    links = content_div.find_all('a', href=True)\n",
    "    \n",
    "    urls = set()\n",
    "    for link in links:\n",
    "        href = link['href']\n",
    "        if 'https' in href:\n",
    "            full_url = href\n",
    "        else:\n",
    "            full_url = 'https://en.wikibooks.org' + href\n",
    "            # print(f'full_url {full_url}')\n",
    "        urls.add(full_url)\n",
    "        # if href.startswith('/wiki/Rocket_Propulsion/') and ':' not in href:\n",
    "        #     print(f'url: https://en.wikibooks.org + { href}')\n",
    "        #     urls.add('https://en.wikibooks.org' + href)\n",
    "\n",
    "    return list(urls)\n",
    "\n",
    "def scrape_pages_with_qg(urls, visited, M, batch_size=10):\n",
    "    \"\"\"Scrape up to `batch_size` new pages not in `visited`, return new QAs.\"\"\"\n",
    "    qa_pairs = []\n",
    "    count = 0\n",
    "    print(f'visited {visited}')\n",
    "\n",
    "    for url in urls:\n",
    "        if url in visited:\n",
    "            continue\n",
    "        visited.add(url)\n",
    "        try:\n",
    "            resp = requests.get(url)\n",
    "            soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "            content_div = soup.select_one('#mw-content-text')\n",
    "            paragraphs = content_div.find_all('p')\n",
    "            text = clean_text(' '.join(p.get_text() for p in paragraphs[:3]))\n",
    "            if not text or len(text) < M:\n",
    "                continue\n",
    "\n",
    "            # Generate question from text\n",
    "            input_text = f\"generate question: {text}\"\n",
    "            output = qg_pipeline(input_text, max_length=64, do_sample=False)[0]['generated_text']\n",
    "\n",
    "            qa_pairs.append({\n",
    "                'question': output,\n",
    "                'answer': text,\n",
    "                'source_url': url\n",
    "            })\n",
    "            print(f\"‚úÖ Generated Q&A from: {url}\")\n",
    "            count += 1\n",
    "            time.sleep(1)  # polite scraping\n",
    "\n",
    "            if count >= batch_size:\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Failed to process {url}: {e}\")\n",
    "    return qa_pairs\n",
    "\n",
    "def scrape_wikibook_qas(M):\n",
    "    all_links = get_all_rocket_propulsion_links()\n",
    "    visited = set()\n",
    "    all_qas = []\n",
    "\n",
    "    for i in range(10):\n",
    "        print(f\"\\nüîÅ Batch {i+1}/10\")\n",
    "        batch_qas = scrape_pages_with_qg(all_links, visited, M, batch_size=10)\n",
    "        all_qas.extend(batch_qas)\n",
    "        if len(all_qas) >= M:\n",
    "            break\n",
    "\n",
    "    # with open('wikibook_qas_100.json', 'w', encoding='utf-8') as f:\n",
    "    #     json.dump(all_qas, f, indent=2)\n",
    "\n",
    "    print(f\"\\n‚úÖ Done. Saved {len(all_qas)} Q&A pairs to wikibook_qas_100.json\")\n",
    "    return all_qas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "719f642f-6bb1-4cb2-8c60-da96865f386e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Stack Exchange API\n",
    "\n",
    "def clean_html_text(html_content):\n",
    "    \"\"\"Remove hyperlinks and strip HTML tags from content.\"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Replace <a> tags with their inner text\n",
    "    for a in soup.find_all('a'):\n",
    "        a.replace_with(a.get_text())\n",
    "\n",
    "    # Get cleaned text\n",
    "    text = soup.get_text(separator=' ')\n",
    "    return html.unescape(text.strip())\n",
    "\n",
    "def fetch_stackexchange_qa_data(site='space.stackexchange', tag='rockets', pagesize=20, max_pages=2):\n",
    "    base_url = 'https://api.stackexchange.com/2.3/questions'\n",
    "    answers_url = 'https://api.stackexchange.com/2.3/questions/{ids}/answers'\n",
    "    all_qas = []\n",
    "\n",
    "    for page in range(1, max_pages + 1):\n",
    "        params = {\n",
    "            'site': site,\n",
    "            'tagged': tag,\n",
    "            'pagesize': pagesize,\n",
    "            'page': page,\n",
    "            'filter': 'withbody'\n",
    "        }\n",
    "        resp = requests.get(base_url, params=params)\n",
    "        data = resp.json()\n",
    "\n",
    "        for question in data.get('items', []):\n",
    "            q_id = question['question_id']\n",
    "            q_body = clean_html_text(question.get('body', ''))\n",
    "            title = html.unescape(question.get('title', ''))\n",
    "\n",
    "            # Get answers\n",
    "            a_params = {\n",
    "                'site': site,\n",
    "                'filter': 'withbody'\n",
    "            }\n",
    "            a_resp = requests.get(answers_url.format(ids=q_id), params=a_params)\n",
    "            answers = a_resp.json().get('items', [])\n",
    "\n",
    "            for ans in answers:\n",
    "                a_body = clean_html_text(ans.get('body', ''))\n",
    "                all_qas.append({\n",
    "                    'question': f\"{title}\\n{q_body}\",\n",
    "                    'answer': a_body,\n",
    "                    'source_url': f\"https://{site}.com/questions/{q_id}\"\n",
    "                })\n",
    "\n",
    "            time.sleep(1)  # polite API usage\n",
    "\n",
    "    return all_qas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9c3507a-3c15-442f-8d5f-7b0a0ce55bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    se_data = fetch_stackexchange_qa_data()\n",
    "    # arxiv_data = fetch_arxiv_abstracts()\n",
    "    # reddit_data = fetch_reddit_qa()\n",
    "    # wikibook_data = scrape_wikibook_qas(M=100)\n",
    "\n",
    "    # all_qas = {\n",
    "    #     \"stackexchange\": se_data,\n",
    "    #     \"arxiv\": arxiv_data,\n",
    "    #     \"reddit\": reddit_data,\n",
    "    #     \"wikibook\": wikibook_data\n",
    "    # }\n",
    "\n",
    "    # save_qas_to_file(all_qas)\n",
    "    # print(\"Data collection complete. Saved to qa_data.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7bae52e4-b8d2-4b66-a834-8ba3d5c432cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The Martian: Does it really take a supercomputer to calculate spaceflight maneuvers?\\nMy preemptive apologies for asking a question about a movie, and the spoilers within said question, but considering the  widespread support for its scientific plausibility , I'm hoping you'll let it slide :) \\n In the movie  The Martian , the character Rich Purnell is shown using the Pleiades supercomputer at the NASA Ames Research Center to confirm the calculations for his maneuver designed to safely redirect the Hermes spacecraft back to Mars, and then to Earth. Why? \\n Space is just about the most ideal place possible for predictable physics. Little in the way of air or external forces, short of gravity which can be calculated between the spacecraft and the Sun and planets and little else, centrifugal force only during the maneuver itself, almost none of the fluid mechanics that otherwise make simple calculations complicated... Basically, with so few moving parts and sources of complexity, does it really take a  250,000-core supercomputer  to run those calculations, or could Rich use, say, his Macbook instead?\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "se_data[5]['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c99c21fa-8f6e-4c3d-8b32-f63e5f50ee22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Martian: Does it really take a supercomputer to calculate spaceflight maneuvers?'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "se_data[5]['question'].split('\\n')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a1afd3-3c67-45b2-8211-9c806fe2cc61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "91f894c2-cae5-49a4-9fcf-0eb77841f13d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I think the answer is probably no, but not for the reasons other answers give.  First of all we can ignore the whole multi-body problem: it's a really good approximation that the planets & Sun  run on rails since they are hugely more massive than the spacecraft.  let's also assume that modelling a trajectory between two points is tractable, whether or not you use continuous thrust or not (this could well be a reasonably hard optimisation problem to minimise fuel &c but I suspect that's very doable on a modern personal computer. \\n That's not what makes it hard: what makes it hard is that this is a search problem merely dressed up as a physics problem, and search problems, famously, have combinatorial explosions.  Search problems require machines like  Deep Blue  to solve them, and these things are definitely supercomputers (albeit specialised ones). \\n Why is is a search problem?  Well, because the way you get around the Solar System isn't in fact by computing a trajectory between two points, it's by computing a bunch of gravitational slingshots around other bodies in the Solar System.  And there are a large number of such possible trajectories, and the number increases, possibly exponentially, as you increase the number of slingshots.  And you can't deform the trajectories into each other to use any nice numerical solving approach because you keep crashing into planets since all these trajectories go rather close to planets. \\n Checking  a proposed trajectory is much easier: if I tell you the plan is to do a couple of assists around Venus, a course correction burn in deep space then an assist around Earth and one around Jupiter on the way to Saturn (this is what Cassini did) then you can pretty easily check the trajectory is OK and compute its fine details.  But  arriving  at such a trajectory is a different question.  This smells strongly of P and NP: given a solution it is easy to check, but arriving at a solution might be hard. \\n So this might actually be a computationally seriously demanding problem.  I think it probably  isn't  in fact, for a few reasons: there aren't very many objects you can use for slingshots so the search space doesn't explode too badly, and the mission duration is constrained as is fuel for course adjustments &c so you can prune solutions which take more time than you have or may need more fuel than you have.  I suspect that keeps the computation sane. \\n \\n [Note I'm posting this answer as a guest: I started writing it on the physics SE last night but the question got migrated & I don't belong to this SE.]\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "se_data[5]['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ca48da5-bcd2-4151-9aab-3ebd3fae4875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Roscosmos <a href=https://arstechnica.com/science/2022/07/russia-says-its-space-station-partnership-will-end-after-two-more-years/ rel=noreferrer>has announced their ISS partnership will end in 2 years</a></p>\\n \\nThis might have been seen as an indication that the new administration at Roscosmos was in a more cooperative mood. Any such hopes were dashed on Tuesday, when Borisov announced that Russia would not be renewing its current commitment to the ISS, which ends in 2024. NASA's current plans involve keeping the station occupied through the end of the decade.</p>\\n \\nIt's not clear what their plans are, but the new head of Roscosmos did say this</p>\\n \\nAccording to The New York Times, Borisov told Russian President Vladimir Putin that the 2024 date gives his country time as well. ‚ÄúI think that by this time, we will begin to form the Russian orbital station,‚Äù he said.</p>\\n \\nIn theory, Russia may want some of its ISS segments back for this &quot;new&quot; Russian space station. That <a href=https://space.stackexchange.com/q/4525/22326>does present a problem</a>, in that we need some of those modules, and a decent chunk of our modules were flown on the now-defunct Space Shuttle.</p>\\nFrom <a href=https://arstechnica.com/science/2022/03/how-to-save-the-international-space-station-and-prevent-the-dreaded-gap/ rel=noreferrer>a different article in March</a>, it seems like <a href=https://blogs.nasa.gov/spacestation/2022/02/21/cygnus-installed-to-station-for-cargo-transfers/ rel=noreferrer>NASA's Cygnus module</a> could be used to replace the Russian boosting capacity. The catch is that the rocket that took it up there (Atlas V) was made with Russian engines and all the ones we still have are spoken for.</p>\\n \\nCygnus has previously launched on the American-made Atlas V rocket. But this booster also uses Russian-made engines. Because of that, the Atlas V was already due to be phased out later this decade after completing two dozen more launches. The Atlas V rocket developer, United Launch Alliance, has taken delivery of all the Russian engines it needs for these flights. Although these missions are all booked, one solution may be for Amazon to give back some of the nine Atlas V launches it has reserved for its Project Kuiper satellite constellation. Another scenario involves launching Cygnus on a Falcon 9 rocket, something Northrop and SpaceX would probably agree upon in an emergency situation.</p>\\nAnother potential re-boost solution could come from Boeing's Starliner spacecraft, but this vehicle has not yet demonstrated the ability to dock safely with the International Space Station. And it, too, is reliant upon launching on the Atlas V rocket.</p>\\n \\nThe SLS is still in testing and, given the enormous cost of a heavy lift non-reusable vehicle, it seems unlikely to be used for the ISS.</p>\\nCould a Falcon Heavy do the trick for another Cygnus, or an entirely new ISS module? Or would any hopes for heavy lift capacity be in rockets not yet built/flown?</p>\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "se_data[1][\"body\"].replace(\"<p>\",\"\").replace('\"','').replace(\"</blockquote>\",\" \").replace(\"<a>\",\"\").replace(\"<blockquote>\",\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b4eace-6457-4b98-8b69-7488d1699ee9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_torch",
   "language": "python",
   "name": "gpu_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
