{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5af1d1e6-3856-4c06-b4ff-4233f6b6c93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "import html\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import xml.etree.ElementTree as ET\n",
    "from transformers import pipeline\n",
    "import sentencepiece\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import io\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0fbfc0f-9e6d-45cd-bd54-bdb2775a7231",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "# Initialize the question generation pipeline\n",
    "qg_pipeline = pipeline(\"text2text-generation\", model=\"valhalla/t5-base-qg-hl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a143cd1-bbf5-4b2a-9475-fb69c6df7490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_split_sentences(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        sentences = f.readlines()\n",
    "    qas = []\n",
    "    for c, item in enumerate(sentences[0:3500]):\n",
    "        input_text = f\"generate question: {item}\"\n",
    "        generated_question = qg_pipeline(input_text, max_length=512, do_sample=False)[0]['generated_text']\n",
    "        yield {\n",
    "            'question': generated_question,\n",
    "            'answer': item,\n",
    "            'source': 'SpaceSystemsDataset'\n",
    "        }\n",
    "        # print(f'{c} -> /n {generated_question}/n {item}')\n",
    "    # return qas\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da2b9994-4945-44b7-99e1-e8cbe260476b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# space_systems_qas = list(load_and_split_sentences('/Users/rckyi/Documents/Data/SpaceSystemsDataset/2 SpaceTransformersCorpus/Sentences_WikiBooksAbstracts.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fea372cb-9717-4bd7-b5a8-ac3c123d3d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(space_systems_qas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b6ed7a1-82f1-471c-a94b-85f5c14157ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_split_paragraphs(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Split on one or more blank lines (handles multiple \\n between paragraphs)\n",
    "    import re\n",
    "    pattern = r\"(*Figure -*\\s*\\s*-\\s*(\\d+)\\s*\"\n",
    "    pat = r\"[ ]*(\\d+[ ]*)|\\(*Figure -*\\s*\\d+\\)?\"\n",
    "    # paragraphs = [para.strip() for para in re.split(r'\\n\\s*\\n', re.sub(pattern, \" \", text)) if para.strip()]\n",
    "    paragraphs = [para.strip().replace(\"- . \",\"\").replace(\" . \",\"\") for para in re.split(r'\\n\\s*\\n', re.sub(r\"\\s*\\d+\\s*\", \"\", re.sub(pat, \" \", text))) if para.strip()]\n",
    "    \n",
    "    return paragraphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5e44546-8bbc-46e9-baad-4dfab4be91e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1393 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "paragraphs = load_and_split_paragraphs(\"/Users/rckyi/Documents/Data/Nasa-Lessons-learned-in-engineering.txt\")\n",
    "\n",
    "lessons_learned_qa = []\n",
    "for i, para in enumerate(paragraphs):  # Print first 5 paragraph\n",
    "    input_text = f\"generate question: {para}\"\n",
    "    generated_question = qg_pipeline(input_text, max_length=512, do_sample=False)[0]['generated_text']\n",
    "    lessons_learned_qa.append({\n",
    "            'question': generated_question,\n",
    "            'answer': para,\n",
    "            'source': 'nasa: lessons learned'\n",
    "    })\n",
    "    \n",
    "    # print(f\"\\n--- Paragraph {i + 1} ---\\n{generated_question}\\n{para}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19289fc8-e301-4cbe-b0b9-65d5a8e8ac0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paragraphs_ps = split_into_paragraphs(\"/Users/rckyi/Documents/Data/A HISTORY OF AEROSPACE PROBLEMS, THEIR SOLUTIONS, THEIR LESSONS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c46c00f8-58a3-4898-bbaa-7204b3db035b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, para in enumerate(paragraphs_ps[200:300]):  # Print first 5 paragraph\n",
    "#     input_text = f\"generate question: {para}\"\n",
    "#     generated_question = qg_pipeline(input_text, max_length=512, do_sample=False)[0]['generated_text']\n",
    "#     print(f\"\\n--- Paragraph {i + 1} ---\\n{generated_question}\\n{para}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e44c1b62-0e7f-49ab-af2c-2f133e282b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_paragraphs(json_file_path):\n",
    "    with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "        paragraphs = json.load(f)\n",
    "\n",
    "    results = []\n",
    "    for para_id, para_text in paragraphs.items():\n",
    "        input_text = f\"generate question: {para_text}\"\n",
    "        results.append({'question': qg_pipeline(input_text, max_length=712, do_sample=False)[0]['generated_text'], \n",
    "                        'answer': para_text,\n",
    "                       'source':  f'nasa: a history of aerospace problems and solns'\n",
    "                       })\n",
    "        # yield [qg_pipeline(input_text, max_length=512, do_sample=False)[0]['generated_text'], para_text]\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f8ab7b4-dcdc-44fa-a66b-e7415c9d6c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "json_file_path = \"/Users/rckyi/Documents/Data/paragraphs_with_ids.json\"  # or your full path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e31ad57-2f6b-4614-b8d1-36be91d8350d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arXiv API Q&A extraction from abstract (pseudo-QA from title and abstract)\n",
    "def fetch_arxiv_abstracts(query='rocket propulsion', max_results=100):\n",
    "    url = f\"http://export.arxiv.org/api/query?search_query=all:{query.replace(' ', '+')}&start=0&max_results={max_results}\"\n",
    "    resp = requests.get(url)\n",
    "    root = ET.fromstring(resp.content)\n",
    "\n",
    "    qas = []\n",
    "    ns = {'atom': 'http://www.w3.org/2005/Atom'}  # arXiv uses Atom XML namespace\n",
    "\n",
    "    for entry in root.findall('atom:entry', ns):\n",
    "        title = entry.find('atom:title', ns).text.strip()\n",
    "        \n",
    "        summary = entry.find('atom:summary', ns).text.strip()\n",
    "        # Generate question from text\n",
    "        input_text = f\"generate question: {summary}\"\n",
    "        generated_question = qg_pipeline(input_text, max_length=64, do_sample=False)[0]['generated_text']\n",
    "        \n",
    "        qas.append({\n",
    "            # 'title': title,\n",
    "            'question': generated_question,\n",
    "            'answer': summary,\n",
    "            'source': f'arxiv'\n",
    "        })\n",
    "    return qas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a46cadbc-7b22-4282-885d-7b7867293821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wikipedia scraping\n",
    "\n",
    "# Initialize the question generation pipeline\n",
    "# qg_pipeline = pipeline(\"text2text-generation\", model=\"valhalla/t5-base-qg-hl\")\n",
    "\n",
    "def clean_text(text):\n",
    "    return ' '.join(text.strip().split())\n",
    "\n",
    "def get_all_rocket_propulsion_links(base_url='https://en.wikibooks.org/wiki/Rocket_Propulsion'):\n",
    "    \"\"\"Grab all unique subpage links under Rocket Propulsion.\"\"\"\n",
    "    resp = requests.get(base_url)\n",
    "    soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "\n",
    "    content_div = soup.select_one('#mw-content-text')\n",
    "    links = content_div.find_all('a', href=True)\n",
    "    \n",
    "    urls = set()\n",
    "    for link in links:\n",
    "        href = link['href']\n",
    "        if 'https' in href:\n",
    "            full_url = href\n",
    "        else:\n",
    "            full_url = 'https://en.wikibooks.org' + href\n",
    "        urls.add(full_url)\n",
    "\n",
    "    return list(urls)\n",
    "\n",
    "def scrape_pages_with_qg(urls, visited, M, batch_size=10):\n",
    "    \"\"\"Scrape up to `batch_size` new pages not in `visited`, return new QAs.\"\"\"\n",
    "    qa_pairs = []\n",
    "    count = 0\n",
    "\n",
    "    for url in urls:\n",
    "        if url in visited:\n",
    "            continue\n",
    "        visited.add(url)\n",
    "        try:\n",
    "            resp = requests.get(url)\n",
    "            soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "            content_div = soup.select_one('#mw-content-text')\n",
    "            paragraphs = content_div.find_all('p')\n",
    "            text = clean_text(' '.join(p.get_text() for p in paragraphs[:3]))\n",
    "            if not text or len(text) < M:\n",
    "                continue\n",
    "\n",
    "            # Generate question from text\n",
    "            input_text = f\"generate question: {text}\"\n",
    "            output = qg_pipeline(input_text, max_length=64, do_sample=False)[0]['generated_text']\n",
    "\n",
    "            qa_pairs.append({\n",
    "                'question': output,\n",
    "                'answer': text,\n",
    "                'source': f\"rocketry wiki\"\n",
    "            })\n",
    "            print(f\"‚úÖ Generated Q&A from: {url}\")\n",
    "            count += 1\n",
    "            time.sleep(1)  # polite scraping\n",
    "\n",
    "            if count >= batch_size:\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Failed to process {url}: {e}\")\n",
    "    return qa_pairs\n",
    "\n",
    "def scrape_wikibook_qas(M):\n",
    "    all_links = get_all_rocket_propulsion_links()\n",
    "    visited = set()\n",
    "    all_qas = []\n",
    "\n",
    "    for i in range(10):\n",
    "        print(f\"\\nüîÅ Batch {i+1}/10\")\n",
    "        batch_qas = scrape_pages_with_qg(all_links, visited, M, batch_size=10)\n",
    "        all_qas.extend(batch_qas)\n",
    "        if len(all_qas) >= M:\n",
    "            break\n",
    "\n",
    "    return all_qas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "719f642f-6bb1-4cb2-8c60-da96865f386e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack Exchange API\n",
    "\n",
    "def clean_html_text(html_content):\n",
    "    \"\"\"Remove hyperlinks and strip HTML tags from content.\"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Replace <a> tags with their inner text\n",
    "    for a in soup.find_all('a'):\n",
    "        a.replace_with(a.get_text())\n",
    "\n",
    "    # Get cleaned text\n",
    "    text = soup.get_text(separator=' ')\n",
    "    return html.unescape(text.strip())\n",
    "\n",
    "def fetch_stackexchange_qas(site='space.stackexchange', tag='rockets', pagesize=20, max_pages=6):\n",
    "    base_url = 'https://api.stackexchange.com/2.3/questions'\n",
    "    answers_url = 'https://api.stackexchange.com/2.3/questions/{ids}/answers'\n",
    "    all_qas = []\n",
    "\n",
    "    for page in range(1, max_pages + 1):\n",
    "        params = {\n",
    "            'site': site,\n",
    "            'tagged': tag,\n",
    "            'pagesize': pagesize,\n",
    "            'page': page,\n",
    "            'filter': 'withbody'\n",
    "        }\n",
    "        resp = requests.get(base_url, params=params)\n",
    "        print(resp)\n",
    "        data = resp.json()\n",
    "\n",
    "        for question in data.get('items', []):\n",
    "            q_id = question['question_id']\n",
    "            q_body = clean_html_text(question.get('body', ''))\n",
    "            title = html.unescape(question.get('title', ''))\n",
    "\n",
    "            # Get answers\n",
    "            a_params = {\n",
    "                'site': site,\n",
    "                'filter': 'withbody'\n",
    "            }\n",
    "            a_resp = requests.get(answers_url.format(ids=q_id), params=a_params)\n",
    "            answers = a_resp.json().get('items', [])\n",
    "\n",
    "            for ans in answers:\n",
    "                a_body = clean_html_text(ans.get('body', ''))\n",
    "                all_qas.append({\n",
    "                    'question': f\"{title}\\n{q_body}\",\n",
    "                    'answer': a_body,\n",
    "                    'source': f\"https://{site}.com/questions/{q_id}\"\n",
    "                })\n",
    "\n",
    "            time.sleep(20)  # polite API usage\n",
    "\n",
    "    return all_qas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fdfb7242-e0f0-4311-b725-cdd7bbd0f22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# se_data = fetch_stackexchange_qas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9c3507a-3c15-442f-8d5f-7b0a0ce55bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "\n",
      "üîÅ Batch 1/10\n",
      "‚úÖ Generated Q&A from: https://en.wikibooks.org/wiki/Space_Transport_and_Engineering_Methods/Engineering_Tools\n",
      "‚úÖ Generated Q&A from: https://en.wikibooks.org/wiki/Space_Transport_and_Engineering_Methods/Environment_Ranges\n",
      "‚úÖ Generated Q&A from: https://en.wikibooks.org/wiki/Space_Transport_and_Engineering_Methods/Methodologies\n",
      "‚úÖ Generated Q&A from: https://en.wikibooks.org/wiki/Space_Transport_and_Engineering_Methods/Existing_Programs\n",
      "‚úÖ Generated Q&A from: https://en.wikibooks.org/wiki/Space_Transport_and_Engineering_Methods/Phase2B\n",
      "‚úÖ Generated Q&A from: https://en.wikibooks.org/wiki/Space_Transport_and_Engineering_Methods/Later_Projects\n",
      "‚úÖ Generated Q&A from: https://en.wikibooks.org/wiki/Space_Transport_and_Engineering_Methods/Starter\n",
      "‚úÖ Generated Q&A from: https://en.wikibooks.org/wiki/Space_Transport_and_Engineering_Methods/Mars_Development\n",
      "‚úÖ Generated Q&A from: https://en.wikibooks.org/wiki/Space_Transport_and_Engineering_Methods/Engineering_Specialties\n",
      "‚úÖ Generated Q&A from: https://en.wikibooks.org/wiki/Space_Transport_and_Engineering_Methods/Orbital_Mining2\n",
      "\n",
      "üîÅ Batch 2/10\n",
      "‚úÖ Generated Q&A from: https://en.wikibooks.org/wiki/Space_Transport_and_Engineering_Methods/Combined_Systems2\n",
      "‚úÖ Generated Q&A from: https://en.wikibooks.org/wiki/Space_Transport_and_Engineering_Methods/Hypervelocity_Launcher\n",
      "‚úÖ Generated Q&A from: https://en.wikibooks.org/wiki/Space_Transport_and_Engineering_Methods/Recycling_Methods\n",
      "‚ö†Ô∏è Failed to process https://drive.google.com/file/d/1PY2VAXLEWCzDNrGQCo-uMRzT_NrnB9ar/view: 'NoneType' object has no attribute 'find_all'\n",
      "‚úÖ Generated Q&A from: https://en.wikibooks.org/wiki/Space_Transport_and_Engineering_Methods/Resource_Uses\n",
      "‚úÖ Generated Q&A from: https://en.wikibooks.org/wiki/Space_Transport_and_Engineering_Methods/Guns_and_Accelerators2\n",
      "‚úÖ Generated Q&A from: https://en.wikibooks.org/wiki/Space_Transport_and_Engineering_Methods/Combined_Systems4\n",
      "‚úÖ Generated Q&A from: https://en.wikibooks.org/wiki/Space_Transport_and_Engineering_Methods/Design_Studies\n",
      "‚úÖ Generated Q&A from: https://en.wikibooks.org/wiki/Space_Transport_and_Engineering_Methods/Operations\n",
      "‚úÖ Generated Q&A from: https://en.wikibooks.org/wiki/Space_Transport_and_Engineering_Methods/Resource_Exploration2\n",
      "‚úÖ Generated Q&A from: https://en.wikibooks.org/wiki/Space_Transport_and_Engineering_Methods/System_Elements\n",
      "\n",
      "üîÅ Batch 3/10\n",
      "‚úÖ Generated Q&A from: https://en.wikibooks.org/wiki/Space_Transport_and_Engineering_Methods/Interplanetary\n",
      "‚úÖ Generated Q&A from: https://en.wikibooks.org/wiki/Space_Transport_and_Engineering_Methods/Engineering_Methods\n",
      "‚úÖ Generated Q&A from: https://en.wikibooks.org/wiki/Space_Transport_and_Engineering_Methods/Phase4B\n",
      "‚úÖ Generated Q&A from: https://en.wikibooks.org/wiki/Space_Transport_and_Engineering_Methods/Energy\n",
      "‚úÖ Generated Q&A from: https://en.wikibooks.org/wiki/Space_Transport_and_Engineering_Methods/Production_Methods\n",
      "‚úÖ Generated Q&A from: https://en.wikibooks.org/wiki/Space_Transport_and_Engineering_Methods/Phase4A\n",
      "‚úÖ Generated Q&A from: https://en.wikibooks.org/wiki/Space_Transport_and_Engineering_Methods/Economics\n",
      "‚úÖ Generated Q&A from: https://en.wikibooks.org/wiki/Space_Transport_and_Engineering_Methods/Combustion_Engines\n",
      "‚úÖ Generated Q&A from: https://en.wikibooks.org/wiki/Space_Transport_and_Engineering_Methods/Space_Elevator\n",
      "‚úÖ Generated Q&A from: https://en.wikibooks.org/wiki/Space_Transport_and_Engineering_Methods/Electric_Propulsion\n",
      "\n",
      "üîÅ Batch 4/10\n",
      "‚úÖ Generated Q&A from: https://en.wikibooks.org/wiki/Space_Transport_and_Engineering_Methods/Comparisons_Among_Methods\n",
      "‚úÖ Generated Q&A from: https://en.wikibooks.org/wiki/Space_Transport_and_Engineering_Methods/Lunar_Development\n",
      "\n",
      "üîÅ Batch 5/10\n",
      "\n",
      "üîÅ Batch 6/10\n",
      "\n",
      "üîÅ Batch 7/10\n",
      "\n",
      "üîÅ Batch 8/10\n",
      "\n",
      "üîÅ Batch 9/10\n",
      "\n",
      "üîÅ Batch 10/10\n",
      "\n",
      "‚úÖ Done. Saved 757 Q&A pairs to qas_data.json\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    arxiv_data = fetch_arxiv_abstracts()\n",
    "    se_data = fetch_stackexchange_qas()\n",
    "    # reddit_data = fetch_reddit_qa()\n",
    "    wikibook_data = scrape_wikibook_qas(M=1000)\n",
    "    # space_systems_qas = list(load_and_split_sentences('/Users/rckyi/Documents/Data/SpaceSystemsDataset/2 SpaceTransformersCorpus/Sentences_WikiBooksAbstracts.txt'))\n",
    "    # nasa_probs_solns = process_paragraphs(json_file_path)\n",
    "    all_qas = arxiv_data + se_data + wikibook_data + lessons_learned_qa\n",
    "\n",
    "    # all_qas = {\n",
    "    #     # \"stackexchange\": se_data,\n",
    "    #     \"arxiv\": arxiv_data,\n",
    "    #     # \"reddit\": reddit_data,\n",
    "    #     \"wikibook\": wikibook_data,\n",
    "    #     \"spacesystems\": space_systems_qas,\n",
    "    #     \"nasa lessons learned\": lessons_learned_qa\n",
    "    #     # \"nasa problems and solns\": nasa_probs_solns\n",
    "    # }\n",
    "\n",
    "    with open('/Users/rckyi/Documents/Data/all_qas_dataset.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_qas, f, indent=2)\n",
    "\n",
    "    print(f\"\\n‚úÖ Done. Saved {len(all_qas)} Q&A pairs to qas_data.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afdf94d-3235-4eb2-8afb-6cbe5b1021ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_torch",
   "language": "python",
   "name": "gpu_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
