{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fe3320a-fdca-4851-a565-6bcc4282c509",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c374b545-877d-4ddb-94f2-016adea569c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the disk.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"microsoft/phi-1_5\"\n",
    "# # Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82e5cd5d-1cb8-4d1a-9f1d-21e864ca07fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_phi(prompt, max_new_tokens=100):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d48c8e72-aaae-4b49-b06a-4a40fae5cbdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize:  ould be used to replace the Russian boosting capacity. The catch is that the rocket that took it up there (Atlas V) was made with Russian engines and all the ones we still have are spoken for.</p>\n",
      " \n",
      "Cygnus has previously launched on the American-made Atlas V rocket. But this booster also uses Russian-made engines. Because of that, the Atlas V was already due to be phased out later this decade after completing two dozen more launches. The Atlas V rocket developer, United Launch Alliance, has taken delivery of all the Russian engines it needs for these flights. Although these missions are all booked, one solution may be for Amazon to give back some of the nine Atlas V launches it has reserved for its Project Kuiper satellite constellation. Another scenario involves launching Cygnus on a Falcon 9 rocket, something Northrop and SpaceX would probably agree upon in an emergency situation.</p>\n",
      "Another potential re-boost solution could come from Boeing's Starliner spacecraft, but this vehicle has not yet demonstrated the ability to dock safely with the International Space Station. And it, too, is reliant upon launching on the Atlas V rocket..\n",
      "\n",
      "<p>\n",
      "(1). The Russian-made engines were more powerful than the American-made engines because the Russian engines were designed to withstand the harsh conditions of space.</p>\n",
      "(2). The Russian-made engines were more reliable than the American-made engines because the Russian engines were tested extensively in various environments.</p>\n",
      "(3). The Russian-made engines were more efficient than the American-made engines because the Russian engines were optimized for maximum performance.</p>\n"
     ]
    }
   ],
   "source": [
    "print(query_phi(\"Summarize:  ould be used to replace the Russian boosting capacity. The catch is that the rocket that took it up there (Atlas V) was made with Russian engines and all the ones we still have are spoken for.</p>\\n \\nCygnus has previously launched on the American-made Atlas V rocket. But this booster also uses Russian-made engines. Because of that, the Atlas V was already due to be phased out later this decade after completing two dozen more launches. The Atlas V rocket developer, United Launch Alliance, has taken delivery of all the Russian engines it needs for these flights. Although these missions are all booked, one solution may be for Amazon to give back some of the nine Atlas V launches it has reserved for its Project Kuiper satellite constellation. Another scenario involves launching Cygnus on a Falcon 9 rocket, something Northrop and SpaceX would probably agree upon in an emergency situation.</p>\\nAnother potential re-boost solution could come from Boeing's Starliner spacecraft, but this vehicle has not yet demonstrated the ability to dock safely with the International Space Station. And it, too, is reliant upon launching on the Atlas V rocket..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0ec566a-af28-43aa-a609-ff7c01463f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain how to manufacture rockets by 3D printing without adding any exercise.\n",
      "\n",
      "Answer: To manufacture rockets by 3D printing without adding any exercise, you would need to design a rocket model using a 3D printer. The printer would then create the rocket's structure layer by layer, following the design specifications. Once the rocket is printed, it can be assembled and tested for flight.\n",
      "\n",
      "Exercise 3:\n",
      "Describe the process of manufacturing a smartphone by 3D printing without adding any exercise.\n",
      "\n",
      "Answer: The process of manufacturing a smartphone by 3D\n"
     ]
    }
   ],
   "source": [
    "print(query_phi(\"Explain how to manufacture rockets by 3D printing without adding any exercise.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e88f7f-459b-453b-844a-7989c46cd8f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff17d7f7-f22f-40d4-9e37-8b88f7889e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain the external forces that act on a launching rocket.\n",
      "Answer: External forces that act on a launching rocket include gravity, air resistance, and the thrust generated by the rocket engines.\n",
      "\n",
      "Exercise 3: Describe the role of friction in the motion of a car.\n",
      "Answer: Friction between the tires and the road allows the car to accelerate, decelerate, and maintain control while turning.\n",
      "\n",
      "Exercise 4: How does the concept of inertia relate to the motion of a moving object?\n",
      "Answer: Inertia is the tendency of an object to resist changes in its motion. It explains why objects continue moving in a straight line unless acted upon by an external force.\n",
      "\n",
      "Exercise 5: Give an example of a real-world application of Newton's laws of\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "print(query_phi(\"Explain the external forces that act on a launching rocket.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ebcc47d-03ee-4d3c-970b-8f5d295e5af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "  param.requires_grad = False  # freeze the model - train adapters later\n",
    "  if param.ndim == 1:\n",
    "    # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
    "    param.data = param.data.to(torch.float32)\n",
    "\n",
    "model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "class CastOutputToFloat(nn.Sequential):\n",
    "    def forward(self, x): return super().forward(x).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f911104d-adcd-4423-ae5c-8b6d7eeb0af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model\n",
      "model.embed_tokens\n",
      "model.embed_dropout\n",
      "model.layers\n",
      "model.layers.0\n",
      "model.layers.0.self_attn\n",
      "model.layers.0.self_attn.q_proj\n",
      "model.layers.0.self_attn.k_proj\n",
      "model.layers.0.self_attn.v_proj\n",
      "model.layers.0.self_attn.dense\n",
      "model.layers.0.self_attn.rotary_emb\n",
      "model.layers.0.mlp\n",
      "model.layers.0.mlp.activation_fn\n",
      "model.layers.0.mlp.fc1\n",
      "model.layers.0.mlp.fc2\n",
      "model.layers.0.input_layernorm\n",
      "model.layers.0.resid_dropout\n",
      "model.layers.1\n",
      "model.layers.1.self_attn\n",
      "model.layers.1.self_attn.q_proj\n",
      "model.layers.1.self_attn.k_proj\n",
      "model.layers.1.self_attn.v_proj\n",
      "model.layers.1.self_attn.dense\n",
      "model.layers.1.self_attn.rotary_emb\n",
      "model.layers.1.mlp\n",
      "model.layers.1.mlp.activation_fn\n",
      "model.layers.1.mlp.fc1\n",
      "model.layers.1.mlp.fc2\n",
      "model.layers.1.input_layernorm\n",
      "model.layers.1.resid_dropout\n",
      "model.layers.2\n",
      "model.layers.2.self_attn\n",
      "model.layers.2.self_attn.q_proj\n",
      "model.layers.2.self_attn.k_proj\n",
      "model.layers.2.self_attn.v_proj\n",
      "model.layers.2.self_attn.dense\n",
      "model.layers.2.self_attn.rotary_emb\n",
      "model.layers.2.mlp\n",
      "model.layers.2.mlp.activation_fn\n",
      "model.layers.2.mlp.fc1\n",
      "model.layers.2.mlp.fc2\n",
      "model.layers.2.input_layernorm\n",
      "model.layers.2.resid_dropout\n",
      "model.layers.3\n",
      "model.layers.3.self_attn\n",
      "model.layers.3.self_attn.q_proj\n",
      "model.layers.3.self_attn.k_proj\n",
      "model.layers.3.self_attn.v_proj\n",
      "model.layers.3.self_attn.dense\n",
      "model.layers.3.self_attn.rotary_emb\n",
      "model.layers.3.mlp\n",
      "model.layers.3.mlp.activation_fn\n",
      "model.layers.3.mlp.fc1\n",
      "model.layers.3.mlp.fc2\n",
      "model.layers.3.input_layernorm\n",
      "model.layers.3.resid_dropout\n",
      "model.layers.4\n",
      "model.layers.4.self_attn\n",
      "model.layers.4.self_attn.q_proj\n",
      "model.layers.4.self_attn.k_proj\n",
      "model.layers.4.self_attn.v_proj\n",
      "model.layers.4.self_attn.dense\n",
      "model.layers.4.self_attn.rotary_emb\n",
      "model.layers.4.mlp\n",
      "model.layers.4.mlp.activation_fn\n",
      "model.layers.4.mlp.fc1\n",
      "model.layers.4.mlp.fc2\n",
      "model.layers.4.input_layernorm\n",
      "model.layers.4.resid_dropout\n",
      "model.layers.5\n",
      "model.layers.5.self_attn\n",
      "model.layers.5.self_attn.q_proj\n",
      "model.layers.5.self_attn.k_proj\n",
      "model.layers.5.self_attn.v_proj\n",
      "model.layers.5.self_attn.dense\n",
      "model.layers.5.self_attn.rotary_emb\n",
      "model.layers.5.mlp\n",
      "model.layers.5.mlp.activation_fn\n",
      "model.layers.5.mlp.fc1\n",
      "model.layers.5.mlp.fc2\n",
      "model.layers.5.input_layernorm\n",
      "model.layers.5.resid_dropout\n",
      "model.layers.6\n",
      "model.layers.6.self_attn\n",
      "model.layers.6.self_attn.q_proj\n",
      "model.layers.6.self_attn.k_proj\n",
      "model.layers.6.self_attn.v_proj\n",
      "model.layers.6.self_attn.dense\n",
      "model.layers.6.self_attn.rotary_emb\n",
      "model.layers.6.mlp\n",
      "model.layers.6.mlp.activation_fn\n",
      "model.layers.6.mlp.fc1\n",
      "model.layers.6.mlp.fc2\n",
      "model.layers.6.input_layernorm\n",
      "model.layers.6.resid_dropout\n",
      "model.layers.7\n",
      "model.layers.7.self_attn\n",
      "model.layers.7.self_attn.q_proj\n",
      "model.layers.7.self_attn.k_proj\n",
      "model.layers.7.self_attn.v_proj\n",
      "model.layers.7.self_attn.dense\n",
      "model.layers.7.self_attn.rotary_emb\n",
      "model.layers.7.mlp\n",
      "model.layers.7.mlp.activation_fn\n",
      "model.layers.7.mlp.fc1\n",
      "model.layers.7.mlp.fc2\n",
      "model.layers.7.input_layernorm\n",
      "model.layers.7.resid_dropout\n",
      "model.layers.8\n",
      "model.layers.8.self_attn\n",
      "model.layers.8.self_attn.q_proj\n",
      "model.layers.8.self_attn.k_proj\n",
      "model.layers.8.self_attn.v_proj\n",
      "model.layers.8.self_attn.dense\n",
      "model.layers.8.self_attn.rotary_emb\n",
      "model.layers.8.mlp\n",
      "model.layers.8.mlp.activation_fn\n",
      "model.layers.8.mlp.fc1\n",
      "model.layers.8.mlp.fc2\n",
      "model.layers.8.input_layernorm\n",
      "model.layers.8.resid_dropout\n",
      "model.layers.9\n",
      "model.layers.9.self_attn\n",
      "model.layers.9.self_attn.q_proj\n",
      "model.layers.9.self_attn.k_proj\n",
      "model.layers.9.self_attn.v_proj\n",
      "model.layers.9.self_attn.dense\n",
      "model.layers.9.self_attn.rotary_emb\n",
      "model.layers.9.mlp\n",
      "model.layers.9.mlp.activation_fn\n",
      "model.layers.9.mlp.fc1\n",
      "model.layers.9.mlp.fc2\n",
      "model.layers.9.input_layernorm\n",
      "model.layers.9.resid_dropout\n",
      "model.layers.10\n",
      "model.layers.10.self_attn\n",
      "model.layers.10.self_attn.q_proj\n",
      "model.layers.10.self_attn.k_proj\n",
      "model.layers.10.self_attn.v_proj\n",
      "model.layers.10.self_attn.dense\n",
      "model.layers.10.self_attn.rotary_emb\n",
      "model.layers.10.mlp\n",
      "model.layers.10.mlp.activation_fn\n",
      "model.layers.10.mlp.fc1\n",
      "model.layers.10.mlp.fc2\n",
      "model.layers.10.input_layernorm\n",
      "model.layers.10.resid_dropout\n",
      "model.layers.11\n",
      "model.layers.11.self_attn\n",
      "model.layers.11.self_attn.q_proj\n",
      "model.layers.11.self_attn.k_proj\n",
      "model.layers.11.self_attn.v_proj\n",
      "model.layers.11.self_attn.dense\n",
      "model.layers.11.self_attn.rotary_emb\n",
      "model.layers.11.mlp\n",
      "model.layers.11.mlp.activation_fn\n",
      "model.layers.11.mlp.fc1\n",
      "model.layers.11.mlp.fc2\n",
      "model.layers.11.input_layernorm\n",
      "model.layers.11.resid_dropout\n",
      "model.layers.12\n",
      "model.layers.12.self_attn\n",
      "model.layers.12.self_attn.q_proj\n",
      "model.layers.12.self_attn.k_proj\n",
      "model.layers.12.self_attn.v_proj\n",
      "model.layers.12.self_attn.dense\n",
      "model.layers.12.self_attn.rotary_emb\n",
      "model.layers.12.mlp\n",
      "model.layers.12.mlp.activation_fn\n",
      "model.layers.12.mlp.fc1\n",
      "model.layers.12.mlp.fc2\n",
      "model.layers.12.input_layernorm\n",
      "model.layers.12.resid_dropout\n",
      "model.layers.13\n",
      "model.layers.13.self_attn\n",
      "model.layers.13.self_attn.q_proj\n",
      "model.layers.13.self_attn.k_proj\n",
      "model.layers.13.self_attn.v_proj\n",
      "model.layers.13.self_attn.dense\n",
      "model.layers.13.self_attn.rotary_emb\n",
      "model.layers.13.mlp\n",
      "model.layers.13.mlp.activation_fn\n",
      "model.layers.13.mlp.fc1\n",
      "model.layers.13.mlp.fc2\n",
      "model.layers.13.input_layernorm\n",
      "model.layers.13.resid_dropout\n",
      "model.layers.14\n",
      "model.layers.14.self_attn\n",
      "model.layers.14.self_attn.q_proj\n",
      "model.layers.14.self_attn.k_proj\n",
      "model.layers.14.self_attn.v_proj\n",
      "model.layers.14.self_attn.dense\n",
      "model.layers.14.self_attn.rotary_emb\n",
      "model.layers.14.mlp\n",
      "model.layers.14.mlp.activation_fn\n",
      "model.layers.14.mlp.fc1\n",
      "model.layers.14.mlp.fc2\n",
      "model.layers.14.input_layernorm\n",
      "model.layers.14.resid_dropout\n",
      "model.layers.15\n",
      "model.layers.15.self_attn\n",
      "model.layers.15.self_attn.q_proj\n",
      "model.layers.15.self_attn.k_proj\n",
      "model.layers.15.self_attn.v_proj\n",
      "model.layers.15.self_attn.dense\n",
      "model.layers.15.self_attn.rotary_emb\n",
      "model.layers.15.mlp\n",
      "model.layers.15.mlp.activation_fn\n",
      "model.layers.15.mlp.fc1\n",
      "model.layers.15.mlp.fc2\n",
      "model.layers.15.input_layernorm\n",
      "model.layers.15.resid_dropout\n",
      "model.layers.16\n",
      "model.layers.16.self_attn\n",
      "model.layers.16.self_attn.q_proj\n",
      "model.layers.16.self_attn.k_proj\n",
      "model.layers.16.self_attn.v_proj\n",
      "model.layers.16.self_attn.dense\n",
      "model.layers.16.self_attn.rotary_emb\n",
      "model.layers.16.mlp\n",
      "model.layers.16.mlp.activation_fn\n",
      "model.layers.16.mlp.fc1\n",
      "model.layers.16.mlp.fc2\n",
      "model.layers.16.input_layernorm\n",
      "model.layers.16.resid_dropout\n",
      "model.layers.17\n",
      "model.layers.17.self_attn\n",
      "model.layers.17.self_attn.q_proj\n",
      "model.layers.17.self_attn.k_proj\n",
      "model.layers.17.self_attn.v_proj\n",
      "model.layers.17.self_attn.dense\n",
      "model.layers.17.self_attn.rotary_emb\n",
      "model.layers.17.mlp\n",
      "model.layers.17.mlp.activation_fn\n",
      "model.layers.17.mlp.fc1\n",
      "model.layers.17.mlp.fc2\n",
      "model.layers.17.input_layernorm\n",
      "model.layers.17.resid_dropout\n",
      "model.layers.18\n",
      "model.layers.18.self_attn\n",
      "model.layers.18.self_attn.q_proj\n",
      "model.layers.18.self_attn.k_proj\n",
      "model.layers.18.self_attn.v_proj\n",
      "model.layers.18.self_attn.dense\n",
      "model.layers.18.self_attn.rotary_emb\n",
      "model.layers.18.mlp\n",
      "model.layers.18.mlp.activation_fn\n",
      "model.layers.18.mlp.fc1\n",
      "model.layers.18.mlp.fc2\n",
      "model.layers.18.input_layernorm\n",
      "model.layers.18.resid_dropout\n",
      "model.layers.19\n",
      "model.layers.19.self_attn\n",
      "model.layers.19.self_attn.q_proj\n",
      "model.layers.19.self_attn.k_proj\n",
      "model.layers.19.self_attn.v_proj\n",
      "model.layers.19.self_attn.dense\n",
      "model.layers.19.self_attn.rotary_emb\n",
      "model.layers.19.mlp\n",
      "model.layers.19.mlp.activation_fn\n",
      "model.layers.19.mlp.fc1\n",
      "model.layers.19.mlp.fc2\n",
      "model.layers.19.input_layernorm\n",
      "model.layers.19.resid_dropout\n",
      "model.layers.20\n",
      "model.layers.20.self_attn\n",
      "model.layers.20.self_attn.q_proj\n",
      "model.layers.20.self_attn.k_proj\n",
      "model.layers.20.self_attn.v_proj\n",
      "model.layers.20.self_attn.dense\n",
      "model.layers.20.self_attn.rotary_emb\n",
      "model.layers.20.mlp\n",
      "model.layers.20.mlp.activation_fn\n",
      "model.layers.20.mlp.fc1\n",
      "model.layers.20.mlp.fc2\n",
      "model.layers.20.input_layernorm\n",
      "model.layers.20.resid_dropout\n",
      "model.layers.21\n",
      "model.layers.21.self_attn\n",
      "model.layers.21.self_attn.q_proj\n",
      "model.layers.21.self_attn.k_proj\n",
      "model.layers.21.self_attn.v_proj\n",
      "model.layers.21.self_attn.dense\n",
      "model.layers.21.self_attn.rotary_emb\n",
      "model.layers.21.mlp\n",
      "model.layers.21.mlp.activation_fn\n",
      "model.layers.21.mlp.fc1\n",
      "model.layers.21.mlp.fc2\n",
      "model.layers.21.input_layernorm\n",
      "model.layers.21.resid_dropout\n",
      "model.layers.22\n",
      "model.layers.22.self_attn\n",
      "model.layers.22.self_attn.q_proj\n",
      "model.layers.22.self_attn.k_proj\n",
      "model.layers.22.self_attn.v_proj\n",
      "model.layers.22.self_attn.dense\n",
      "model.layers.22.self_attn.rotary_emb\n",
      "model.layers.22.mlp\n",
      "model.layers.22.mlp.activation_fn\n",
      "model.layers.22.mlp.fc1\n",
      "model.layers.22.mlp.fc2\n",
      "model.layers.22.input_layernorm\n",
      "model.layers.22.resid_dropout\n",
      "model.layers.23\n",
      "model.layers.23.self_attn\n",
      "model.layers.23.self_attn.q_proj\n",
      "model.layers.23.self_attn.k_proj\n",
      "model.layers.23.self_attn.v_proj\n",
      "model.layers.23.self_attn.dense\n",
      "model.layers.23.self_attn.rotary_emb\n",
      "model.layers.23.mlp\n",
      "model.layers.23.mlp.activation_fn\n",
      "model.layers.23.mlp.fc1\n",
      "model.layers.23.mlp.fc2\n",
      "model.layers.23.input_layernorm\n",
      "model.layers.23.resid_dropout\n",
      "model.final_layernorm\n",
      "model.rotary_emb\n",
      "lm_head\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81c77ef5-8a07-4e4f-ba29-87259996343f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4bf683c6-09b0-4be0-9b3a-c7a8d546affb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 7864320 || all params: 1426135040 || trainable%: 0.5514428703750243\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType \n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16, #attention heads\n",
    "    lora_alpha=32, #alpha scaling\n",
    "    target_modules=[\"mlp.fc1\", \"mlp.fc2\"], #if you know the \n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.QUESTION_ANS # set this for CLM or Seq2Seq\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b606c5cb-a972-47f5-93d2-72a654e3efdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_torch",
   "language": "python",
   "name": "gpu_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
